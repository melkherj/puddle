\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2016
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2016}

\usepackage{nips_2016}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2016}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{cite}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{bbm}
\usepackage{microtype}      % microtypography
\usepackage{graphicx}
\usepackage{algorithm,algorithmicx,algpseudocode}

\graphicspath{ {images/} }

\title{Active Calibration for Mini-Batch Semi-Supervised Learning}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  Matthew Elkherj\thanks{Use footnote for providing further
    information about author (webpage, alternative
    address)---\emph{not} for acknowledging funding agencies.} \\
  Department of Computer Science\\
  Cranberry-Lemon University\\
  Pittsburgh, PA 15213 \\
  \texttt{hippo@cs.cranberry-lemon.edu} \\
  %% examples of more authors
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
In this paper we lay out a model-agnostic and computationally efficient framework for semi-supervised active learning.  The algorithm discussed assumes only access to a minibatch-fit and scoring oracle, efficiently scores samples, and runs semi-supervised active learning incrementally.  By combining pseudo-labeling with a novel adaptive calibration algorithm, we achieve empirical results that are state of the art on 10-class-MNIST: 89.5\% held-out accuracy with 85 labels, and 92.5\% with 150 labels.  
\end{abstract}

\section{Introduction}
While there have been great strides in supervised classification problems in text, speech, and audio, recent deep-learning approaches typically require thousands to tens or hundreds of thousands of labeled examples to achive state-of-the-art performance.  Due to the substantial investment required in collecting such sets of labels, the set of problems tackled in industry are only those large enough to justify the cost.  Any simple techniques that can decrease this labeling cost would thus significantly increase the breadth of supervised machine-learning applications currently tackled.  

This is one of the primary motivators for the machine-learning subfields of active, semi-supervised, and transfer learning.  The field of transfer-learning uses a model trained on one dataset to bootstrap a model on another[9][10].  Semi-supervised learning seeks to extract patterns from examples that aren't labeled, and use those patterns discovered to improve performance on labeled examples.  Finally, active learning decreases the number of labels used by intelligently selecting examples to label.  We focus on current techniques in scalable, model-agnostic semi-supervised and active-learning, point out deficiencies and areas for work, then outline our approach that combines new techniques.  We'll then focus on the details of the active and semi-supervised techniques used, an algorithm detailing this interaction, and the results of experiments.  

The field of semi-supervised learning is a relatively old and well-trodden field, with techniques ranging from the broadly applicable to the more-often model-specific.  A few classes of semi-supervised learning approaches include graph methods, clustering, probabilistic models/EM, self/co-training, and density-avoiding-methods[8][6].  

Graph methods like label propagation, min-cut, and conjugate gradient generally operate on a similarity matrix.  In some cases this can be a sparse kNN-similarity-matrix, but the most straightforward techniques can require the construction of a dense matrix, and take total complexity $O(n^3)$.  There are techniques that potentially bring this down to $O(n)$ for sparse graphs, but these still require some specification of a model-specific similarity function[12].  

We focus instead on variants of semi-supervised techniques that are simpler, more scalable and model-agnostic.  These include probabilistic models over scores/EM, and self-training/pseudo-labeling.  Self-training and pseudo-labeling get closest to the objective we're looking to hit[11][1], and can be thought of as a hard (binarized) version of probabilistic/EM methods.  

Self-training generally starts with ground-truth labeled sets, trains a model, and produces pseudo-labels to re-train on.  Model-agnostic variants of EM operate in a similar manner, producing scores instead of hard labelings.  The technique of pseudo-labeling is quite simple in implementation, but can results in substantial empirical gains can be thought of as solving more complex implicit optimization problems.

While striking a balance of simplicty and performance, these techniques aren't without their drawbacks.  One problem with self-training/pseudo-labeling is in determining the set of examples to pseudo-label, and how these trade off with real labels in optimizing an objective.  The scheduling of labeling, pseudo-label-fitting, and scoring is also non-trivial.  Naive approaches are computationally inefficient (score every iteration).  A naive score-cutoff can as well produce more noise that out-weights the additional structured signal.  We use adaptive/active labeling to help outline this boundary, and more efficiently adjust scores / train the model.  We as well outline a specific mini-batch scheduling of active labeling, pseudo-labeling, and scoring that empirically performs well.   

To find this score cutoff, we find ourselves in another area of research focused on label-efficiency: active learning[13].  The techniques here as well have typically required in-depth access to model information or complex model schedule-training[14], but some can be model-agnostic[15].   

In tackling semi-supervised and active techniques simultaneously, we as well need consider techniques that solve for both score calibration and information gain.  We apply simple strategies to mitigate the common problem of bias: balanced-minibatch-sampling and the addition of sampling noise, and demonstrate that the same dynamic thresholding used for active sampling can carry over to pseudo-labeling.  
We choose to focus on pool-based active learning over streaming for this work, but due to the nature of the incremental algorithm expect it could be ported to a streaming learning environment.  

\section{Minibatch Active Semi-supervised Learning}
In this section, we introduce an oracle abstraction for a class of active pool-based semi-supervised learning algorithms, and discuss a particular implementation of an active semi-supervised learning algorithm in this abstraction.

\subsection{A Simple Active Semi-Supervised Oracle}
Here we define an oracle setup in which to implement our active, semi-supervised learning algorithm.  We assume that we're given a pool of inputs, each $x \in \mathbb{R}^{d}$, and can request labels each $y\in\{0..c\}$.  As active/semi-supervised learning frameworks are typically structured, the dual objectives are to train and optimize some objective over the population dataset, while minimizing the number of labels requested.  

In keeping this learning algorithm simple, we restrict it to run as follows.  The algorithm must run as a sequence of epochs.  In each epoch, the algorithm:
\begin{enumerate}
\item Produces a set of examples to request labels for
\item Produces a set of example/label pairs the model scores with sufficient confidence (pseudo-label-examples)
\item Bootstrap samples a minibatch from the semisupervised pseudo-labels and active labels collected so far.
\item Trains/updates on this bootstrap sample.
\item Re-scores a subset of examples for the next epoch
\end{enumerate}

This approach then strikes a balance between pool-based and streaming active learning algorithms.  As is normally the case, if we assume scoring / retraing makes up the bulk of runtime for iterative learning algorithms.  From the approach above, each epoch only need score a small sample, re-train on a minibatch, and label a few cases.  Ensuring latency of requesting the next label is often important for collecting labels in real-time labels.  

In the following sections we'll outline the specific supervised model architecture, semi-supervised, active, and scoring algorithms used.  We'll then combine these into a single algorithm.  

\subsection{Deep Model Architecture}
Since our aim is for example selection and semi-supervised labeling to be agnostic to the supervised model architecture chosen, we don't focus heavily on the supervised model architecture.  For evaluation on MNIST we thus chose a commonly used architecture.  The 28x28 input images are fed into a 32-bank of 5x5 convolutional features, rectified linear and 2x2 max pooling layers, then 5x5x32 convolution with 32 features, relu/max pooling, and finally a 2-level fully connected network with a 1014-variable hidden layer.  For optimization, the adam optimizer was used with a learning rate of $10^-4$.  These settings are defaults used in tensorflow's advanced examples[7].

\subsection{Thresholded Pseudo-Label}
The semi-supervised algorithm we use is similar to that in Pseudo-Label[1], but diverges in a few important respects.  Pseudo-labeling is a simple enough method: to leverage unlabeled examples, we use the current model to label examples and treat those labels as real labels.  In the original paper this was shown to work well, but was highly sensitive to dynamic-balancing of the pseudo and real labels' contribution to overall loss.  We as well went without the deep-auto-encoder phase used in Pseudo-label, since this would break our requirement that the algorithm be model-agnostic.  

To combat this, and for theoretical reasons listed below, we instead label only unsupervised examples with scores above a threshold.  Real and pseudo-labels are given uniform weight in the final loss function, but pseudo-labeling is only done for examples having probability score >0.9.  We show below that empirically, the results using this approach are substantially better than random unsupervised labeling.  

One lense to view thresholded pseudo-labeling from is as graph-score-propagation over an implicitely calculated similarity metric.  Assume we have two samples $x_1,x_2 \in \mathbb{R}^d$.  Assume that both $x_1,x_2$ have ground-truth label $c$, but only the label for $x_1$ has been collected.  After training for enough iterations on $x_1$, we'd expect the model scoring function $f_{\theta}$ to score high in class $c$ for $x_1$.  Assuming $f$ is locally differentiable near $x_1$, then for some $\epsilon$ if $\|x_1-x_2\|<\epsilon$ then $\|f(x_1)-f(x_2)\|$ should be small.  In such a case $f_{\theta}(x_2)$ would be as well, and thus would have a high score for class $c$.  

This analogy has a few theoretical flaws: convolutional networks aren't guaranteed to be locally differentiable due to rectified linear and max pooling layers.  As well, for the argument to fully hold we'd need the differentiability condition to hold for the map and its inverse.  With only one direction valid, we can state that we will capture and learn from cases that are similar, but can't say we won't flag cases that are far from the current labeled set.  Even with its theoretical flaws, in practice this interpretation should often hold.  High-scoring examples that are dissimilar are likely still matches, and so these are useful to include for expanding the examplem set in more than a simple euclidean similarity graph.  It's reasonable to assume that most examples' neurons aren't perfectly at the boundary of a max or relu nonlinearity.  

A benefit of the induced similarity measure is that it should weight inputs according to predictive power, rather than uniformly as a euclidean/cosine/jaccard similarity would do.  Another is that with a high enough threshold, the added noise of using pseudo-labels will be significantly lower, since more labels will be correct. 

This gets us to choosing a threshold.  Our active sampling strategy effectively calibrates a subset of classifier scores, so we leverage this calibration to set a dynamic threshold.  For the remainder of this subsection we will only consider the max score associated with each example and a given model.  We show empirically below that for a small number of labels the deep model we're using is poorly calibrated.  We model calibration error as being made up of just a bias term before being passed to the final sigmoid, and under this assumption give a method to adjust.  In logistic regression or our deep net, that score is $\sigma(\hat{z})$ for some input $\hat{z}$, $\sigma(x)=\frac{1}{1+e^{-x}}$.  We assume that our model score $\sigma(\hat{z})$ is systematically off the ground-truth model's prediction of $\sigma(z)$ by $\hat{z}=z+b$, $b$ the bias.  

It's convenient that a prerequisite for the active sampling strategy used is estimating the bias term $b$.  We can then use that term to obtain a modified threshold $\tau$ that has calibrated probability 0.9.  Given $\tau$ is a threshold with biased scores achieving this 0.9 calibrated probability, we have that $\sigma(\hat{z})=\tau \iff \sigma(z)=0.9$.  Since $\hat{z}=z+b$, we have:
$\sigma(\hat{z}-b)=0.9$, $\hat{z}=\sigma^{-1}(0.9)+b$, and thus $\tau_s=\sigma(\sigma^{-1}(0.9)+b)$.  Since we usually deal with bias thresholds, given the 0.5 probability threshold for active learning is $\tau_a$, $b=\sigma^{-1}(\tau_a)$, we have:
$$\tau_s=\sigma(\sigma^{-1}(0.9)+\sigma^{-1}(\tau_a))$$

\subsection{Adaptive Calibration for Active Learning}
From the previous section we've learned that adaptive label selection serves a dual-purpose: for intelligently thresholded semi-supervised learning, and as well as in optimally improving the model with labels.  This helps determine our approach: a calibrated variant of uncertainty-sampling with noise. 

While uncertainty-sampling has been shown to perform worse than QBC and other methods, the effect of calibration to combat bias hasn't been studied to the author's knowledge.  We show that for small sample-sizes where the classifier isn't yet calibrated, adaptively-calibrated uncertainty-sampling beats out naive uncertainty sampling.  We also show that it very efficiently achieves calibration near the score of 0.5. 

A commonly used and effective classifier calibration technique is platt scaling, which fits a logistic regression model with uncalibrated scores as inputs.  The pervasive use and effectiveness justifies our assumption that the functional form (a sigmoid) is not what results in our poorly-calibrated classifier, but bias or parameters.  This gets us back to the assumption of the previous section: that a bias term is what makes up mis-calibration of scores.  

We want to sample near the score threshold of $\tau$, and around this threshold the probability of a positive should be around 0.5.  From earlier we assume that our poorly-calibrated model scores $\hat{z}$ satisfy $\hat{z} = z+b$, $b$ the bias term.  If we find the bias term $b$ and sample around the region $\tau = \sigma(b)$, we should get correct matches with probability 0.5.  

To achieve this, we dynamically approximate the logodds objective and use online gradient descent.  Under the assumption our current bias term achieves calibration, we should have that samples around score $\sigma(z)$ satisfy $z \approx loggodds$, logodds being the empirical logodds from sampling examples around each score $\sigma(z)$.  This gets us $\hat{z}-b \approx log(\frac{p}{n})$.  We then set our objective to be:

$$min_b \left(\hat{z}-b-logodds\right)^2$$

This gets us to a gradient update $b \leftarrow b - \left( logodds+b-\hat{z} \right)$.  Since in our case we're sampling in the region of 0.5 (with biased noise), we have that $b \approx \hat{z}$.  This gets us to the final form:

$$b \leftarrow b - logodds$$

This leaves us with the task of empirically approximating $logodds$ at each timestep.  The model changes each iteration, and so does the labeling distribution.  An easier statistic to approximate than logodds is the proportion of positives.  To bias towards the more recent and correct model, we use an exponentially-decaying count for positives and negatives.  

This gives us at each timestep, our pseudo-count of correct labels is $pos=(1-\gamma)\mathbbm{1}(correct)+\gamma pos$, $neg=(1-\gamma)\mathbbm{1}(correct)+\gamma neg$ for negatives.  

Tuning $\gamma$ is then a balance between significance and low-variance of model estimates, and the recency of the model/score selection logodds being evaluated.  A larger $\gamma$ takes into account more samples in calculating the fraction of positives/negatives, but doesn't adapt as quickly to changes in the underlying model.  

Roughly 10 examples is close to the smallest number of samples to sufficiently approximate a bernouli paremeter around 0.5.  These 10 samples results in a standard deviation of $\sqrt{\frac{p(1-p)}{n}} = 0.5/sqrt(10) \approx 0.15$.  In aiming for 0.99 proportion of our weight to come from the most recent 10, we then arrive at $\gamma \approx 0.65$ as a sane first choice of the hyperparameter.  This is heuristic, and with multiple sessions of active learning could be tuned empirically.  

The last consideration in active learning is combatting bias.  We use a simple tactic here: adding decaying noise to uncertainty sampling.  Working off the recurring theme of gaussian additions to scores before being passed through the final sigmoid, we add decaying gaussian noise to the pre-sigmoid scores.  

This gets us our final sampled scores:
$$\tau' = \sigma(\sigma^{-1}(\tau)+\delta)$$

Where $\delta \sim  \mathcal{N}(0,\alpha e^{-\beta | A | })$.  Here the variance simply decays exponentially in the number of labels.  Again some reasonable apriori choices are sufficient.  Assuming that we'd like variance to be at around 1.5 at 100 iterations (recall $\sigma^{-1}(0.9) \approx 2.2$), and around 0.01 at 100 iterations, we solve and get $\alpha=1.5, \beta=0.05$. 

\section{Algorithm Details}
At this point we have the main background to give a complete algorithm for training, scoring, semi-supervised, and active learning.  To reduce complexity we omitted a full specification of the deep network.  See Algorithm 1.

\begin{algorithm}
\caption{Full minibatch training algorithm}
\label{alg:the_alg}
\begin{algorithmic}[1]
\Require Example pool $X\in\mathbb{R}^{nxd}$, example ids $\mathcal{E}=\{1..n\}$
\Require label oracle $\mathcal{O}:\mathcal{E}\mapsto\{0..c\}$
\State Hyperparameters $m=2000, \delta_2=2.2, \alpha=1.5, \beta=0.05, \tau_0=0.5, \gamma=0.65, \eta_1=0.1,\eta_2=0.1$ \Comment{These parameters were chosen apriori}
\State $A \sim_{c} \mathcal{E} $ \Comment{c random examples to seed label set}
\State $\theta \leftarrow \mathcal{N}(0,1)^{k}$ \Comment{Initialize all model parameters,num params}
\State $\tau \leftarrow \tau_0 $ \Comment{active selection threshold}
\State $P \leftarrow uniform(0,1)^{nxd}$ \Comment{Initialize scores randomly and normalize}
\State $pos,neg \leftarrow 1$ \Comment{1 vs. 0 for laplace smoothing}
\For{$epoch \leftarrow 1 .. m$}
\State 
\State //Active
\If{prob<$\eta_1$} \Comment{Active sample every few epochs}
\State sample $\delta_1 \sim  \mathcal{N}(0,\alpha e^{-\beta | A | })$ \Comment{shrinking noise}
\State $\tau' \leftarrow \sigma(\sigma^{-1}(\tau)+\delta_1)$ \Comment{noised threshold}
\State $i \leftarrow argmin_{i \in E \setminus A} \{ abs(P_{max}(i) - \tau')\}$ \Comment{Pick example to label}
\State $Y_i \leftarrow \mathcal{O}(i)$ \Comment{Query oracle for label}
\State $A = A \cup \left\{ i \right\}$ \Comment{closest example to noised threshold}
\State $pos \leftarrow (1-\gamma)\mathbbm{1}\{predict(i)=Y_i\} + \gamma pos$
\State $neg \leftarrow (1-\gamma)\mathbbm{1}\{predict(i) \neq Y_i\} + \gamma neg$
\State $\tau = \sigma(\sigma^{-1}(\tau) - \mathrm{ log}(\frac{pos}{neg}))$ \Comment{adjust threshold}
\EndIf
\State 
\State //Semisupervised
\State $\tau_s \leftarrow \sigma(\sigma^{-1}(\tau)+\delta_2)$ \Comment{Get roughly probability of 0.9}
\State $B \leftarrow \left\{b \mid model(b)> \tau_s \right\}$ \Comment{Select examples above that threshold}
\For{$i \leftarrow B$}
\State $Y_i \leftarrow predict_{\theta} (i)$
\EndFor
\State
\State //Model Update
\State $a \leftarrow \textrm{balanced-sample}(A,40)$
\State $b \leftarrow \textrm{balanced-sample}(B,40)$
\State $\theta \leftarrow \textrm{minibatch-fit}(X,Y,a \cup b)$
\State
\State //Rescore
\State Update model scores $\{ P_i \mid i \in \textrm{uniform-sample}(E,\eta_2) \}$
\EndFor
\end{algorithmic}
\end{algorithm}

Walking through the algorithm, we see that it follows the general form outlined in the oracle abstraction earlier.  Variables are instantiated, then a process of sampling/labeling, semisupervised learning, model training, and scoring runs in minibatches.  

The core of this algorithm is striking a balance in the progress of each of the component sections.  The components to balance are the amounts of model updating, rescoring, and active sampling per epoch.  The amount of semisupervised learning is controlled by active calibration, and an apriori-set parameter $\delta_3$ (see derivation above).  

The rates of progress are controlled by the parameters $\eta_1$,$\eta_2$, and the number 40 in the model update section.  Since it's the relative quantities of parameters that matter most, and somewhere in the range of 40-100 is a generally-recognized-good parameter setting for MNIST minibatch-sizes, we fix the MNIST minibatch size first.  

This leaves us with $\eta_1$ and $\eta_2$.  As with other parameters we've discussed, it's easy to come up with a very reasonable apriori number that will give solid performance.  Starting with the labeling probability: we must leave time for the model to converge, and semi-supervised labels to propagate, before.  The minimum time we could imagine giving there is 10 iterations, hence 0.1.  

The point of this algorithm is for it to be efficient, and so finding the minimum amount of re-scoring to get results.  A sampling fraction of 0.1 is the simplest one could imagine, and in expectation re-scores 95\% of the dataset for every 3 active labels.  We tried 0.01 as well, but empirical results were significantly worse.  



\section{Experiments}
The core experiments we ran were on MNIST, using the well-known deep-network architecture detailed above.  Data was collected from these experiments to compare the accuracy per label of algorithm variants, and evaluate the effectiveness of the above calibration algorithm.  

\subsection{Calibration and sampling}
We found that the adaptive calibration algorithm detailed above was quite effective, although there could still be work in reducing overshooting, and achieving convergence guarantees.  

Two trials were required for the calibration experiments.  In both the semi-supervised learning algorithm pseudo-labeled above a threshold as defined in previous sections, but in one the threshold wasn't adjusted and no noise was added (straight 0.5 uncertainty-sampling), and in the other the threshold changed.  Comparing the rolling 10-label-window fraction of positives between the two is quite clear.  

In both trials, the trend up to 100 labels is easily explained.  The model has random weights and hasn't learned.  Thus at any but the highest thresholds the fraction of positives will be less than 0.5, and model scores overestimate propensity.  The trend above 0.5 after 100 labels is more unusual.  This could be a case of bias in the classifier induced by active/pseudo-labeling.  See Figure 1.  

\begin{figure}[h]
  \center 
  \includegraphics[scale=0.5]{adaptive-threshold}
  \includegraphics[scale=0.5]{calibration}
  \includegraphics[scale=0.5]{non-adaptive}
  \caption{Left: Adaptive threshold over number of labels queried.  It's interesting to note that in starting out, we would expect performance to be very poor, so almost all scores (even those around 0.5) would be over-estimates.  
  Right: empirical fraction of selected labels with adaptive thresholding that are positive.  This is pretty close to calibrated throughout, the notable exception being around 100 labels.  Note that all plots start after 10, since we using rolling averages over the last 10 labels.  
  Bottom: This is the baseline with uncertainty-sampling: selecting examples with raw score 0.5.  As expected less than 0.5 fraction start out positive, but what's more interesting is that scores then overshoot.  }
\end{figure}
\subsection{Held-out Accuracy}
In Figure 2, we see the core results of threshold-pseudo-labeling and adaptive calibration/selection.  There were 6 experiments run against the same MNIST dataset with the same deep network.  As explained, the naming convention followed label-selector/semisupervised-selector.  The random active selector is the easiest to grasp.  In the Algorithm1, instead of maintaining thresholds and selecting labels based on those, we uniformly randomly choose the next example to request a label of.  The "uncertain" sampling strategy requests labels for examples that have a max class-score closest to 0.5.  Adaptive label-sampling is using the main sampling strategy we detailed above.

For the semi-supervised selectors, "random" uniformly randomly selects pseudo-labels in each mini-batch.  Empty does no semi-supervised learning.  The "top" selector is the threshold semi-supervised selector detailed in Algorithm 1.  

It's clear that adaptive/uncertainty sampling, along with semi-supervised-learning based on thresholding performs the much better than the rest.  At around 85 labels, adaptive sampling with threshold-pseudo-labeling gets to around 90\%.  Most other methods don't reach that performance until around 200 labels.  

It's interesting to see the performance of adaptive and uncertainty sampling even out at around 80 labels.  One possible explanation here is from the previous calibration plots: at around 80 labels the adaptive calibration scores take a rapid dive.  This rapid dive is to combat the increase in performance from when the classifier begins making gains.  An interesting follow-up here would be to analyze higher probability thresholds than 0.5, as that could as well explain some of these interesting artifacts.  



\begin{figure}[h]
  \center 
  \includegraphics[scale=0.5]{mnist-results}
  \caption{Performance of various active-semisupervised algorithm variants.  These follow the naming convention <label selector>-<semisupervised selector>}
\end{figure}

\section{Conclusion}
From this work we can draw a few clear conclusions, but there are plenty of avenues to dig into in follow-ups.  The most compelling conclusion is that using simple model-agnostic algorithms, it's possible to get state-of-the-art performance in very few labels.  

The main avenues of further research would be to better characterize patterns in results: precisely when and why does adaptive calibration and pseudo-labeling perform well?  As well, theoretical work on convergence of adaptive calibration, and simulations could be helpful for practitioners interested in calibration for its own right.

Another obvious avenue to apply these methods is to highly-skewed-labels: MNIST is nearly perfectly balanced.  One would expect the performance gains and problems faced to be significantly different for highly-skewed classes.  The benefits over random sampling should also be much more compelling.  

Lastly, the application and evaluation of these techniques on other datasets/models would probably shed light on a number of the questions above.  The most straightforward would be character-level convolutional network classification, but any supervised problem would be interesting.  
\section*{References}

\small

[1] Lee, Dong-Hyun. "Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks." Workshop on Challenges in Representation Learning, ICML. Vol. 3. 2013.

[2] Sutton, Richard S. "Learning to predict by the methods of temporal differences." Machine learning 3.1 (1988): 9-44.

[3] Weston, Jason, et al. "Deep learning via semi-supervised embedding." Neural Networks: Tricks of the Trade. Springer Berlin Heidelberg, 2012. 639-655.

[4] Ganti, Ravi, and Alexander Gray. "Upal: Unbiased pool based active learning." arXiv preprint arXiv:1111.1784 (2011).

[5] Zhu, Xiaojin, John Lafferty, and Zoubin Ghahramani. "Combining active learning and semi-supervised learning using gaussian fields and harmonic functions." ICML 2003 workshop on the continuum from labeled to unlabeled data in machine learning and data mining. Vol. 3. 2003.

[6] McCallumzy, Andrew Kachites, and Kamal Nigamy. "Employing EM and pool-based active learning for text classification." Proc. International Conference on Machine Learning (ICML). 1998.

[7] Abadi, MartÄ±n, et al. "TensorFlow: Large-scale machine learning on heterogeneous systems, 2015." Software available from tensorflow. org.

[8] Zhu, Xiaojin. "Semi-supervised learning literature survey." (2005).

[9] Pan, Sinno Jialin, and Qiang Yang. "A survey on transfer learning." Knowledge and Data Engineering, IEEE Transactions on 22.10 (2010): 1345-1359.

[10] Bengio, Yoshua. "Deep learning of representations for unsupervised and transfer learning." Unsupervised and Transfer Learning Challenges in Machine Learning 7 (2012): 19.

[11] Rosenberg, Chuck, Martial Hebert, and Henry Schneiderman. "Semi-supervised self-training of object detection models." (2005).

[12] Sindhwani, Vikas, Partha Niyogi, and Mikhail Belkin. "Beyond the point cloud: from transductive to semi-supervised learning." Proceedings of the 22nd international conference on Machine learning. ACM, 2005.

[13] Settles, Burr. "Active learning literature survey." University of Wisconsin, Madison 52.55-66 (2010): 11.

[14] Seung, H. Sebastian, Manfred Opper, and Haim Sompolinsky. "Query by committee." Proceedings of the fifth annual workshop on Computational learning theory. ACM, 1992.

[15] Balcan, Maria-Florina, Alina Beygelzimer, and John Langford. "Agnostic active learning." Proceedings of the 23rd international conference on Machine learning. ACM, 2006.

\end{document}

Online Calibration for Semi-Supervised Active Learning 
