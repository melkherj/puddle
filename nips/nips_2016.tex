\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2016
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2016}

\usepackage{nips_2016}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2016}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{cite}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{bbm}
\usepackage{microtype}      % microtypography
\usepackage{graphicx}
\usepackage{algorithm,algorithmicx,algpseudocode}

\graphicspath{ {images/} }

\title{Sequential, Pool-based Semi-supervised Active Learning}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  Matthew Elkherj\thanks{Use footnote for providing further
    information about author (webpage, alternative
    address)---\emph{not} for acknowledging funding agencies.} \\
  Department of Computer Science\\
  Cranberry-Lemon University\\
  Pittsburgh, PA 15213 \\
  \texttt{hippo@cs.cranberry-lemon.edu} \\
  %% examples of more authors
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
In this paper we lay out a model-agnostic and computationally efficient framework for semi-supervised active learning.  The algorithm assumes only access to a model minibatch-fit and scoring oracle, efficiently scores samples, and runs semi-supervised active learning incrementally on mini-batches.  We achieve empirical results that are state of the art on 10-class-MNIST: 89.5\% held-out accuracy with 85 labels, and 92.5\% with 150 labels.  
\end{abstract}

\section{Introduction}
While there have been great strides in supervised classification problems in text, speech, and audio, recent deep-learning approaches typically require thousands or tens of thousands of labeled examples to achive state-of-the-art performance.  Due to the substantial investment required in collecting such a set of labels, the set of problems tackled in industry are those large enough to justify the cost.  Any simple techniques that can decrease this labeling cost would thus significantly increase the breadth of supervised machine-learning applications currently tackled.  

This is one of the primary motivators for the machine-learning subfields of active, semi-supervised, and transfer learning.  The field of transfer-learning uses a model trained on one dataset, to bootstrap a model on another[9][10].  Semi-supervised learning seeks to extract patterns from examples that aren't labeled, and use those patterns discovered to improve performance on labeled examples.  Finally, active learning decreases the number of labels used by intelligently selecting examples to label.  

A few classes of semi-supervised learning approaches include graph methods, clustering, and probabilistic/density-based sampling[8][6].  These methods generally require deep model introspection, modeling assumptions, or the creation of an $O(n^2)$ similarity-matrix.  Graph-based methods utilitize a similarity matrix, constructed via domain-knowledge or a model.  Methods utilizing probabilistic graphical models assume data follows that particular distribution.  

Active learning algorithms .  We chose to focus on pool-based active learning.  

We present an algorithm that 
Active learning as well falls into 
Modern semi-supervised learning approaches generally fall into 
Some of the most commonly-used classes algorithms for semi-supervised learning are 
Semi-supervised algorithms
<common semisupervised learning techniques>
<common active learning techniques>  <why pool over active>
<fully describe why our result is radically different from the above, why we aim at semi-supervised and active>
We give a model-agnostic algorithm that achieves state-of-the-art results, 

\section{Minibatch Active Semi-supervised Learning}
In this section, we introduce an oracle abstraction for a class of active pool-based semi-supervised learning algorithms, and a few classes of algorithms to interface with such an oracle.  

\subsection{A Simple Active Semi-Supervised Oracle}
Here we define an oracle setup in which to implement our active, semi-supervised learning algorithm.  We assume that we're given a pool of inputs, each $x \in \mathbb{R}^{d}$, and can request labels each $y\in\{0..c\}$.  As active learning frameworks are typically structured, the dual objectives are to train and optimize some objective over the population dataset, while minimizing the number of labels requested.  

In keeping this learning algorithm simple, we restrict it to run as follows.  The algorithm must run as a sequence of epochs.  In each epoch, the algorithm:
\begin{enumerate}
\item Produces a set of examples to request labels for
\item Produces a set of example/label pairs from model would output with sufficient confidence
\item Bootstrap samples a minibatch from the semisupervised pseudo-labels and active labels collected so far.
\item Re-scores a small, random subset of examples
\end{enumerate}

This approach then strikes a balance between pool-based and streaming active learning algorithms.  As is normally the case, if we assume scoring / retraing makes up the bulk of runtime for iterative learning algorithms.  From the approach above, each epoch only need score a small sample, re-train on a minibatch, and label a few cases.  Ensuring latency of requesting the next label is often important for collecting labels in real-time labels.  

\subsection{Deep Model Architecture}
The point of this approach is for the example selection and semi-supervised labeling to be agnostic to the supervised model architecture chosen.  For evaluation on MNIST we thus chose a commonly used architecture.  The 28x28 input images are fed into a 32-bank of 5x5 convolutional features, rectified linear and 2x2 max pooling layers, then 5x5x32 convolution with 32 features, relu/max pooling, and finally a 2-level fully connected network with a 1014-variable hidden layer.  For optimization, the adam optimizer was used with a learning rate of $10^-4$.  These settings are defaults used in tensorflow's advanced examples[7].

\subsection{Thresholded Pseudo-Label}
The semi-supervised algorithm we use is similar to that in Pseudo-Label[1], but diverges in a few important respects.  Pseudo-labeling is a simple enough method: to leverage unlabeled examples, we use the current model to label examples and treat those labels as real labels.  In the original paper this was shown to work well, but was highly sensitive to dynamic-balancing of the pseudo and real labels' contribution to overall loss.  We as well went without the deep-auto-encoder phase used in Pseudo-label, since this would break our requirement that the algorithm be model-agnostic.  

To combat this, and for theoretical reasons listed below, we instead label only unsupervised examples with scores above a threshold.  Real and pseudo-labels are given uniform weight in the final loss function, but pseudo-labeling is only done for examples having probability score >0.9.  We show below that empirically, the results using this approach are substantially better than random unsupervised labeling.  

One lense to view thresholded pseudo-labeling from is as graph-score-propagation over an implicitely calculated similarity metric.  Assume we have two samples $x_1,x_2 \in \mathbb{R}^d$.  Assume that both $x_1,x_2$ have ground-truth label $c$, but only the label for $x_1$ has been collected.  After training for enough iterations on $x_1$, we'd expect the model scoring function $f_{\theta}$ to score high in class $c$ for $x_1$.  Assuming $f$ is locally differentiable near $x_1$, then for some $\epsilon$ if $\|x_1-x_2\|<\epsilon$ then $\|f(x_1)-f(x_2)\|$ should be small.  In such a case $f_{\theta}(x_2)$ would be as well, and thus would have a high score for class $c$.  

This analogy has a few theoretical flaws: convolutional networks aren't guaranteed to be locally differentiable due to rectified linear and max pooling layers.  As well, for the argument to fully hold we'd need the differentiability condition to hold for the map and its inverse.  With only one direction valid, we can state that we will capture and learn from cases that are similar, but can't say we won't flag cases that are far from the current labeled set.  It's reasonable to assume that most examples' neurons aren't perfectly at the boundary of a max or relu nonlinearity. 

A positive characteristic of the induced similarity measure is that it should weight inputs according to predictive power, rather than uniformly as a euclidean/cosine/jaccard similarity would do.  Another is that with a high enough threshold, the added noise of using pseudo-labels will be significantly lower, since more labels will be correct. 

This gets us to choosing a threshold.  Our active sampling strategy effectively calibrates a subset of classifier scores, so we leverage this calibration to set a dynamic threshold.  We show empirically below that for a small number of labels the deep model is poorly calibrated.  We model error as being made up of just a bias term before being passed to the final sigmoid.  Concretely, we from now on only consider the max score associated with each example and a given model.  In logistic regression or our deep net, that score is $\sigma(\hat{z})$ for some input $\hat{z}$, $\sigma(x)=\frac{1}{1+e^{-x}}$.  We assume that our model score $\sigma(\hat{z})$ is systematically off the ground-truth model's prediction of $\sigma(z)$ by $\hat{z}=z+b$, $b$ the bias.  

It's quite convenient that a prerequisite for the active sampling strategy used is estimating the bias term $b$.  We can then use that term to obtain a modified threshold $\tau$ that has calibrated probability 0.9.  t such that $\sigma(\hat{z})=t \iff \sigma(z)=0.9$.  Since $\hat{z}=z+b$, we have:
$\sigma(\hat(z)-b)=0.9$, $\hat(z)=\sigma^{-1}(0.9)+b$, and thus $\tau_s=\sigma(\sigma^{-1}(0.9)+b)$.  Since we usually deal with bias thresholds, given the 0.5 probability threshold for active learning is $\tau_a$, $b=\sigma^{-1}(\tau_a)$ and finally we have:
$$\tau_s=\sigma(\sigma^{-1}(0.9)+\sigma^{-1}(\tau_a))$$

\subsection{Adaptive Calibration for Active Learning}
From the previous section we've learned that adaptive label selection serves a dual-purpose: for intelligently thresholded semi-supervised learning, and as well as in optimally improving the model with labels.  This helps determine our approach: a calibrated variant of uncertainty-sampling with noise. 

While uncertainty-sampling has been shown to perform worse than QBC and other methods, the effect of calibration over bias hasn't been studied to the author's knowledge.  We show that for small sample-sizes where the classifier isn't yet calibrated, adaptively-calibrated uncertainty-sampling beats out naive uncertainty sampling.  We also show that it very efficiently achieves calibration near the score of 0.5. 

A commonly used and effective classifier calibration technique is platt scaling, which fits a logistic regression model with uncalibrated scores as inputs.  The pervasive use and effectiveness justifies our assumption that the functional form (a sigmoid) is not what results in our poorly-calibrated classifier, but bias or parameters.  This gets us back to the assumption of the previous section: that a bias term is what makes up mis-calibration of scores.  

We want to sample near the score threshold of $\tau$, and around this threshold the probability of a positive should be around 0.5.  From earlier we assume that our poorly-calibrated model scores $\hat{z}$ satisfy $\hat{z} = z+b$, $b$ the bias term.  If we find the bias term $b$ and sample around the region $\tau = \sigma(b)$, we should get correct matches with probability 0.5.  

To achieve this, we dynamically approximate the logodds objective and use online gradient descent.  Under the assumption our current bias term achieves calibration, we should have that samples around score $\sigma(z)$ satisfy $z \approx loggodds$, logodds being the empirical logodds from sampling examples around each score $\sigma(z)$.  This gets us $\hat{z}-b \approx log(\frac{p}{n})$.  We then set our objective to be:

$$min_b \left(\hat{z}-b-logodds\right)^2$$

This gets us to a gradient update $b \leftarrow b - \gamma \left( logodds+b-\hat{z} \right)$.  Since in our case we're sampling in the region of 0.5 (with biased noise), we have that $b \approx \hat{z}$.  This gets us to the final form:

$$b \leftarrow b - \gamma logodds$$

This leaves us with the task of empirically approximating $logodds$ at each timestep.  The model changes each iteration, and so does the labeling distribution.  An easier statistic to approximate than logodds is the proportion of positives.  To bias towards the more recent and correct model, we use an exponentially-decaying count for positives and negatives.  

This gives us at each timestep, our pseudo-count of correct labels is $neg=(1-\gamma)\mathbbm{1}(correct)+\gamma neg$, $neg=(1-\gamma)\mathbbm{1}(correct)+\gamma neg$ for negatives.  

Tuning $\gamma$ is then a balance between significance and low-variance of model estimates, and the recency of the model/score selection logodds being evaluated.  A larger $\gamma$ takes into account more samples in calculating the fraction of positives/negatives, but doesn't adapt as quickly to changes in the underlying model.  

Around 10 examples is around the lowest we could go to a sane binomial fraction around 0.5, with standard deviation of $\sqrt(\frac{p(1-p)}{n}) = 0.5/sqrt(10) \approx 0.15$.  In aiming for 0.99 proportion of our weight to come from the most recent 10, we then arrive at $\gamma \approx 0.65$ as a sane first choice of the hyperparameter.  This is heuristic, and with multiple sessions of active learning could be tuned empirically.  

The last consideration in active learning is sampling variance decay.  This is a simple tactic to avoid heavy bias: add noise to uncertainty sampling.  Working off the recurring theme of gaussian additions to scores before being passed through the final sigmoid, we add decaying gaussian noise to the pre-sigmoid scores.  

This comes gives us sampled scores:
$$\tau' = \sigma(\sigma^{-1}(\tau)+\delta)$$

Where $\delta \sim  \mathcal{N}(0,\alpha e^{-\beta | A | })$.  Here the variance simply decays exponentially in the number of labels.  Again some reasonable apriori choices are sufficient.  Assuming that we'd like variance to be at around 1.5 at 100 iterations (recall $\sigma^{-1}(0.9) \approx 2.2$), and around 0.01 at 100 iterations, we solve and get $\alpha=1.5, \beta=0.05$. 

\section{Algorithm Details}
At this point we have the main background to give a complete algorithm for training, scoring, semi-supervised, and active learning.  To reduce complexity we omitted a full specification of the deep network.  See Algorithm 1.

\begin{algorithm}
\caption{Full minibatch training algorithm}
\label{alg:the_alg}
\begin{algorithmic}[1]
\Require Example pool $X\in\mathbb{R}^{nxd}$, example ids $\mathcal{E}=\{1..n\}$
\Require label oracle $\mathcal{O}:\mathcal{E}\mapsto\{0..c\}$
\State Hyperparameters $m=2000, \delta_2=2.2, \alpha=1.5, \beta=0.05, \tau_0=0.5, \gamma=0.65, \eta_1=0.1,\eta_2=0.1$ \Comment{These parameters were chosen apriori}
\State $A \sim_{c} \mathcal{E} $ \Comment{c random examples to seed label set}
\State $\theta \leftarrow \mathcal{N}(0,1)^{k}$ \Comment{Initialize all model parameters,num params}
\State $\tau \leftarrow \tau_0 $ \Comment{active selection threshold}
\State $P \leftarrow uniform(0,1)^{nxd}$ \Comment{Initialize scores randomly and normalize}
\State $pos,neg \leftarrow 1$ \Comment{1 vs. 0 for laplace smoothing}
\For{$epoch \leftarrow 1 .. m$}
\State 
\State //Active
\If{prob<$\eta_1$} \Comment{Active sample every few epochs}
\State sample $\delta_1 \sim  \mathcal{N}(0,\alpha e^{-\beta | A | })$ \Comment{shrinking noise}
\State $\tau' \leftarrow \sigma(\sigma^{-1}(\tau)+\delta_1)$ \Comment{noised threshold}
\State $i \leftarrow argmin_{i \in E \setminus A} \{ abs(P_{max}(i) - \tau')\}$ \Comment{Pick example to label}
\State $Y_i \leftarrow \mathcal{O}(i)$ \Comment{Query oracle for label}
\State $A = A \cup \left\{ i \right\}$ \Comment{closest example to noised threshold}
\State $pos \leftarrow (1-\gamma)\mathbbm{1}\{predict(i)=Y_i\} + \gamma pos$
\State $neg \leftarrow (1-\gamma)\mathbbm{1}\{predict(i) \neq Y_i\} + \gamma neg$
\State $\tau = \sigma(\sigma^{-1}(\tau) - \mathrm{ log}(\frac{pos}{neg}))$ \Comment{adjust threshold}
\EndIf
\State 
\State //Semisupervised
\State $\tau_s \leftarrow \sigma(\sigma^{-1}(\tau)+\delta_2)$
\State $B \leftarrow \left\{b \mid model(b)> \tau_s \right\}$ 
\For{$i \leftarrow B$}
\State $Y_i \leftarrow predict_{\theta} (i)$
\EndFor
\State
\State //Model Update
\State $a \leftarrow \textrm{balanced-sample}(A,40)$
\State $b \leftarrow \textrm{balanced-sample}(B,40)$
\State $\theta \leftarrow \textrm{minibatch-fit}(X,Y,a,b)$
\State
\State //Rescore
\State Update model scores $\{ P_i \mid i \in \textrm{uniform-sample}(E,\eta_2) \}$
\EndFor
\end{algorithmic}
\end{algorithm}

Walking through the algorithm, we see that it follows the general form outlined in the oracle earlier.  Variables are instantiated, then a process of sampling/labeling, semisupervised learning, model training, and scoring runs in minibatches.  

The core of this algorithm is striking a balance in the progress of each of the component parts.  The components to balance are the amounts of model updating, rescoring, and active sampling per epoch.  The amount of semisupervised learning is really controlled by active calibration, and an apriori-set parameter $\delta_3$ (see derivation above).  

The rates of progress are controlled by the parameters $\eta_1$,$\eta_2$, and the number 40 in the model update section.  Since it's the relative quantities of parameters that matter most, and somewhere in the range of 40-100 is a generally-recognized-good parameter setting for MNIST minibatch-sizes, we fix the MNIST minibatch size first.  

This leaves us with $\eta_1$ and $\eta_2$.  As with other parameters we've discussed, it's easy to come up with a very reasonable apriori number that will give solid performance.  Starting with the labeling probability: we must leave time for the model to converge, and semi-supervised labels to propagate, before.  The minimum time we could imagine giving there is 10 iterations, hence 0.1.  

The point of this algorithm is for it to be efficient, and so finding the minimum amount of re-scoring to get results.  A sampling fraction of 0.1 is the simplest one could imagine, and in expectation re-scores 95\% of the dataset for every 3 active labels.  We tried 0.01 as well, but empirical results were significantly worse.  



\section{Experiments}
The core experiment we ran was on MNIST, using a well-known deep-network architecture detailed above.  Data was collected from these experiments to compare the accuracy per label of algorithm variants, and evaluate the effectiveness of the above calibration algorithm.  

\subsection{Calibration and sampling}
\begin{figure}[h]
  \center 
  \includegraphics[scale=0.5]{adaptive-threshold}
  \includegraphics[scale=0.5]{calibration}
  \caption{Left: Adaptive threshold over number of labels queried.  Right:empirical fraction of selected labels, with adaptive thresholding, that are positive}
\end{figure}
\subsection{Held-out Accuracy}
\begin{figure}[h]
  \center 
  \includegraphics[scale=0.5]{mnist-results}
  \caption{Performance of various active-semisupervised algorithm variants.  These follow the naming convention <active selector>-<semisupervised selector>}
\end{figure}

What was 


Given this framework, what is left for 

The above framework supports a
While the above framework does restrict the learning algorithm and implementation
The restrictions on our learning algorithm are only then that it can be efficiently trained in mini-batches, 

it has sufficient confidence in to use for semi-supervised learning
objective is to simultaneously minimize the number of labels request
we s with any active learning algorithm, the objective is to 

FUPS: 
- theoretical work on this adaptive calibration algorithm
- deadling with unbalanced sets

\section{Conclusion}
Future work would probably include transfer learning, more datasets (text), 
We expect performance to be more significant for highly-skewed classes.  
Theoretical work on adaptive calibration (right now it's rough).  Regret bounds, ...
It might be interesting in future work to investigate actively re-scoring, based on current score.  
<transfer learning?>
\section*{References}

\small

[1] Lee, Dong-Hyun. "Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks." Workshop on Challenges in Representation Learning, ICML. Vol. 3. 2013.

[2] Sutton, Richard S. "Learning to predict by the methods of temporal differences." Machine learning 3.1 (1988): 9-44.

[3] Weston, Jason, et al. "Deep learning via semi-supervised embedding." Neural Networks: Tricks of the Trade. Springer Berlin Heidelberg, 2012. 639-655.

[4] Ganti, Ravi, and Alexander Gray. "Upal: Unbiased pool based active learning." arXiv preprint arXiv:1111.1784 (2011).

[5] Zhu, Xiaojin, John Lafferty, and Zoubin Ghahramani. "Combining active learning and semi-supervised learning using gaussian fields and harmonic functions." ICML 2003 workshop on the continuum from labeled to unlabeled data in machine learning and data mining. Vol. 3. 2003.

[6] McCallumzy, Andrew Kachites, and Kamal Nigamy. "Employing EM and pool-based active learning for text classification." Proc. International Conference on Machine Learning (ICML). 1998.

[7] Abadi, Martın, et al. "TensorFlow: Large-scale machine learning on heterogeneous systems, 2015." Software available from tensorflow. org.

[8] Zhu, Xiaojin. "Semi-supervised learning literature survey." (2005).

[9] Pan, Sinno Jialin, and Qiang Yang. "A survey on transfer learning." Knowledge and Data Engineering, IEEE Transactions on 22.10 (2010): 1345-1359.

[10] Bengio, Yoshua. "Deep learning of representations for unsupervised and transfer learning." Unsupervised and Transfer Learning Challenges in Machine Learning 7 (2012): 19.







\end{document}


\begin{algorithm}
\caption{}
\begin{algorithmic}
\REQUIRE $n \geq 0 \vee x \neq 0$
\ENSURE $y = x^n$
\STATE $y \leftarrow 1$
\IF{$n < 0$}
\STATE $X \leftarrow 1 / x$
\STATE $N \leftarrow -n$
\ELSE
\STATE $X \leftarrow x$
\STATE $N \leftarrow n$
\ENDIF
\WHILE{$N \neq 0$}
\IF{$N$ is even}
\STATE $X \leftarrow X \times X$
\STATE $N \leftarrow N / 2$
\ELSE[$N$ is odd]
\STATE $y \leftarrow y \times X$
\STATE $N \leftarrow N - 1$
\ENDIF
\ENDWHILE
\end{algorithmic}
\end{algorithm}



























































$$x \in \mathbb{R}^d$$



Online Calibration for Semi-Supervised Active Learning 


\begin{verbatim}



fraction to score = 1-(frac to miss)^expected num iters

NOTICE: our approach is incredibly computationally-efficent (minibatch incremental over pool, vs. O(n) in memory or O(n^2).  such an approch is necessary), simple, and achieves superior performance.  
Naive methods achieve very poor calibration.  We use a label-efficient adaptive calibration algorithm, and a reformulation of pseudo-labeling to achieve state-of-the-art results on image and text classification benchmarks.  

THE NOVELTY:
- reducing pool-based active/semi-supervised learning to a thresholding/selection framework (compare to random)
- algorithms for effectively doing this
- the algorithm re-trains adaptively with examples coming in, training on the pool efficiently
we re-balance!

SECTIONS (6 pages to start, I'm sure there will be more):
- title/abstract
- 1 intro:
    - what are some prior approaches to semisup?  active?  how is what we're doing novel in a nutshell?
- 2 A simple, flexible framework for semi-supervised active learning
- 3 Implementations that work well
- 4 Theoretical justification
- 5 Experiments
    - performance evaluation, setup, the specific deep network structure/topology, 
        
- Discussion

Key contributions:
- model agnostic.  Especially important for deep learning.  
- calibration
- labeling all performs strictly worse

plots:
semisupervised threshold vs. accuracy plots (significant)
significant difference between "incorrect & correct"

KEY FACTS: 
- massive difference in "predicted correctly" vs. not
- adaptively calibrate based on this?  keep at ~0.3 correct empirically?
adaptive vs. passive calibration?
active metric is a combo of: 
- 0.05-bin-density
- probability
compare to uncertainty-sampling:
- just a function of score-value (0.9, ...)


class skew (downsample training?)  keep accuracy as our metric
\end{verbatim}


















\section{Submission of papers to NIPS 2016}

\textbf{There is a new style file for papers submitted in 2016!}

NIPS requires electronic submissions.  The electronic submission site
is
\begin{center}
  \url{https://cmt.research.microsoft.com/NIPS2016/}
\end{center}

Please read carefully the instructions below and follow them
faithfully.



\subsection{Style}

Papers to be submitted to NIPS 2016 must be prepared according to the
instructions presented here. Papers may only be up to eight pages
long, including figures. Since 2009 an additional ninth page
\emph{containing only acknowledgments and/or cited references} is
allowed. Papers that exceed nine pages will not be reviewed, or in any
other way considered for presentation at the conference.

The margins in 2016 are the same as since 2007, which allow for
$\sim$$15\%$ more words in the paper compared to earlier years.

Authors are required to use the NIPS \LaTeX{} style files obtainable
at the NIPS website as indicated below. Please make sure you use the
current files and not previous versions. Tweaking the style files may
be grounds for rejection.

\subsection{Retrieval of style files}

The style files for NIPS and other conference information are
available on the World Wide Web at
\begin{center}
  \url{http://www.nips.cc/}
\end{center}
The file \verb+nips_2016.pdf+ contains these instructions and
illustrates the various formatting requirements your NIPS paper must
satisfy.

The only supported style file for NIPS 2016 is \verb+nips_2016.sty+,
rewritten for \LaTeXe{}.  \textbf{Previous style files for \LaTeX{}
  2.09, Microsoft Word, and RTF are no longer supported!}

The new \LaTeX{} style file contains two optional arguments:
\verb+final+, which creates a camera-ready copy, and \verb+nonatbib+,
which will not load the \verb+natbib+ package for you in case of
package clash.

At submission time, please omit the \verb+final+ option. This will
anonymize your submission and add line numbers to aid review.  Please
do \emph{not} refer to these line numbers in your paper as they will
be removed during generation of camera-ready copies.

The file \verb+nips_2016.tex+ may be used as a ``shell'' for writing
your paper. All you have to do is replace the author, title, abstract,
and text of the paper with your own.

The formatting instructions contained in these style files are
summarized in Sections \ref{gen_inst}, \ref{headings}, and
\ref{others} below.

\section{General formatting instructions}
\label{gen_inst}

The text must be confined within a rectangle 5.5~inches (33~picas)
wide and 9~inches (54~picas) long. The left margin is 1.5~inch
(9~picas).  Use 10~point type with a vertical spacing (leading) of
11~points.  Times New Roman is the preferred typeface throughout, and
will be selected for you by default.  Paragraphs are separated by
\nicefrac{1}{2}~line space (5.5 points), with no indentation.

The paper title should be 17~point, initial caps/lower case, bold,
centered between two horizontal rules. The top rule should be 4~points
thick and the bottom rule should be 1~point thick. Allow
\nicefrac{1}{4}~inch space above and below the title to rules. All
pages should start at 1~inch (6~picas) from the top of the page.

For the final version, authors' names are set in boldface, and each
name is centered above the corresponding address. The lead author's
name is to be listed first (left-most), and the co-authors' names (if
different address) are set to follow. If there is only one co-author,
list both author and co-author side by side.

Please pay special attention to the instructions in Section \ref{others}
regarding figures, tables, acknowledgments, and references.

\section{Headings: first level}
\label{headings}

All headings should be lower case (except for first word and proper
nouns), flush left, and bold.

First-level headings should be in 12-point type.

\subsection{Headings: second level}

Second-level headings should be in 10-point type.

\subsubsection{Headings: third level}

Third-level headings should be in 10-point type.

\paragraph{Paragraphs}

There is also a \verb+\paragraph+ command available, which sets the
heading in bold, flush left, and inline with the text, with the
heading followed by 1\,em of space.

\section{Citations, figures, tables, references}
\label{others}

These instructions apply to everyone.

\subsection{Citations within the text}

The \verb+natbib+ package will be loaded for you by default.
Citations may be author/year or numeric, as long as you maintain
internal consistency.  As to the format of the references themselves,
any style is acceptable as long as it is used consistently.

The documentation for \verb+natbib+ may be found at
\begin{center}
  \url{http://mirrors.ctan.org/macros/latex/contrib/natbib/natnotes.pdf}
\end{center}
Of note is the command \verb+\citet+, which produces citations
appropriate for use in inline text.  For example,
\begin{verbatim}
   \citet{hasselmo} investigated\dots
\end{verbatim}
produces
\begin{quote}
  Hasselmo, et al.\ (1995) investigated\dots
\end{quote}

If you wish to load the \verb+natbib+ package with options, you may
add the following before loading the \verb+nips_2016+ package:
\begin{verbatim}
   \PassOptionsToPackage{options}{natbib}
\end{verbatim}

If \verb+natbib+ clashes with another package you load, you can add
the optional argument \verb+nonatbib+ when loading the style file:
\begin{verbatim}
   \usepackage[nonatbib]{nips_2016}
\end{verbatim}

As submission is double blind, refer to your own published work in the
third person. That is, use ``In the previous work of Jones et
al.\ [4],'' not ``In our previous work [4].'' If you cite your other
papers that are not widely available (e.g., a journal paper under
review), use anonymous author names in the citation, e.g., an author
of the form ``A.\ Anonymous.''

\subsection{Footnotes}

Footnotes should be used sparingly.  If you do require a footnote,
indicate footnotes with a number\footnote{Sample of the first
  footnote.} in the text. Place the footnotes at the bottom of the
page on which they appear.  Precede the footnote with a horizontal
rule of 2~inches (12~picas).

Note that footnotes are properly typeset \emph{after} punctuation
marks.\footnote{As in this example.}

\subsection{Figures}

All artwork must be neat, clean, and legible. Lines should be dark
enough for purposes of reproduction. The figure number and caption
always appear after the figure. Place one line space before the figure
caption and one line space after the figure. The figure caption should
be lower case (except for first word and proper nouns); figures are
numbered consecutively.

You may use color figures.  However, it is best for the figure
captions and the paper body to be legible if the paper is printed in
either black/white or in color.
\begin{figure}[h]
  \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
  \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
  \caption{Sample figure caption.}
\end{figure}

\subsection{Tables}

All tables must be centered, neat, clean and legible.  The table
number and title always appear before the table.  See
Table~\ref{sample-table}.

Place one line space before the table title, one line space after the
table title, and one line space after the table. The table title must
be lower case (except for first word and proper nouns); tables are
numbered consecutively.

Note that publication-quality tables \emph{do not contain vertical
  rules.} We strongly suggest the use of the \verb+booktabs+ package,
which allows for typesetting high-quality, professional tables:
\begin{center}
  \url{https://www.ctan.org/pkg/booktabs}
\end{center}
This package was used to typeset Table~\ref{sample-table}.

\begin{table}[t]
  \caption{Sample table title}
  \label{sample-table}
  \centering
  \begin{tabular}{lll}
    \toprule
    \multicolumn{2}{c}{Part}                   \\
    \cmidrule{1-2}
    Name     & Description     & Size ($\mu$m) \\
    \midrule
    Dendrite & Input terminal  & $\sim$100     \\
    Axon     & Output terminal & $\sim$10      \\
    Soma     & Cell body       & up to $10^6$  \\
    \bottomrule
  \end{tabular}
\end{table}

\section{Final instructions}

Do not change any aspects of the formatting parameters in the style
files.  In particular, do not modify the width or length of the
rectangle the text should fit into, and do not change font sizes
(except perhaps in the \textbf{References} section; see below). Please
note that pages should be numbered.

\section{Preparing PDF files}

Please prepare submission files with paper size ``US Letter,'' and
not, for example, ``A4.''

Fonts were the main cause of problems in the past years. Your PDF file
must only contain Type 1 or Embedded TrueType fonts. Here are a few
instructions to achieve this.

\begin{itemize}

\item You should directly generate PDF files using \verb+pdflatex+.

\item You can check which fonts a PDF files uses.  In Acrobat Reader,
  select the menu Files$>$Document Properties$>$Fonts and select Show
  All Fonts. You can also use the program \verb+pdffonts+ which comes
  with \verb+xpdf+ and is available out-of-the-box on most Linux
  machines.

\item The IEEE has recommendations for generating PDF files whose
  fonts are also acceptable for NIPS. Please see
  \url{http://www.emfield.org/icuwb2010/downloads/IEEE-PDF-SpecV32.pdf}

\item \verb+xfig+ "patterned" shapes are implemented with bitmap
  fonts.  Use "solid" shapes instead.

\item The \verb+\bbold+ package almost always uses bitmap fonts.  You
  should use the equivalent AMS Fonts:
\begin{verbatim}
   \usepackage{amsfonts}
\end{verbatim}
followed by, e.g., \verb+\mathbb{R}+, \verb+\mathbb{N}+, or
\verb+\mathbb{C}+ for $\mathbb{R}$, $\mathbb{N}$ or $\mathbb{C}$.  You
can also use the following workaround for reals, natural and complex:
\begin{verbatim}
   \newcommand{\RR}{I\!\!R} %real numbers
   \newcommand{\Nat}{I\!\!N} %natural numbers
   \newcommand{\CC}{I\!\!\!\!C} %complex numbers
\end{verbatim}
Note that \verb+amsfonts+ is automatically loaded by the
\verb+amssymb+ package.

\end{itemize}

If your file contains type 3 fonts or non embedded TrueType fonts, we
will ask you to fix it.
